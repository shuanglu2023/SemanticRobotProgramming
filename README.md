# Semantic Robot Programming by Demonstration and Natural Language Instruction

## ðŸ§  Description
This repository contains the materials and implementation developed for my PhD thesis *"Semantic Robot Programming by Demonstration and Natural Language Instruction"*.  
The research investigates how robots can learn and execute manipulation tasks by combining **visual demonstrations** and **language-based instructions** into a unified **semantic task model**.

Key contributions include:
- A multimodal framework integrating **vision**, **language**, and **motion** data for robot task understanding.  
- Methods for **object and hand detection**, **trajectory segmentation**, and **motion primitive learning** using **Dynamic Motion Primitives (DMPs)**.  
- A **semantic task model** that connects perception and action for adaptive and generalizable robot behavior.  
- Validation in **ROS/Gazebo simulation**.

## ðŸš€ Usage
- Use `programming.py` to **record the demonstration** through visual and motion capture inputs.  
- Use `execution.py` to **validate the learned task** and execute it on the robot or in simulation.  
- This repository is **under construction** â€” more details, examples, and documentation will follow.

